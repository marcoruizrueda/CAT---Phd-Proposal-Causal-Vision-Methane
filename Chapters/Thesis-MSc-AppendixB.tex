% #############################################################################
% This is Appendix B
% !TEX root = ../main.tex
% #############################################################################
\chapter{Algorithms Implementation}
\label{chapter:appendixB}

This appendix presents the complete implementation of the algorithms used in the methane detection and analysis pipeline. The code is organized according to the workflow steps described in the main text. All scripts and additional implementation details are available in the public repository: \url{https://github.com/marcoruizrueda-ist/PHD}.

\section{Step 1: Data Extraction from NetCDF Files}
\label{sec:appendixB_step1}

The first step involves extracting methane and carbon monoxide measurements from the WFMD (Wetland Flux and Methane Dataset) NetCDF files. This algorithm processes files in parallel to handle the large dataset efficiently.

\begin{lstlisting}[language=Python, caption=Parallel NetCDF Data Extraction Algorithm, label=alg:step01_extract, breaklines=true]
#!/usr/bin/env python
"""
Parallel NetCDF extractor for WFMD satellite data.
Extracts methane and carbon monoxide measurements from large NetCDF files
using parallel processing for improved performance.

Usage: python step01_extract.py --workers 200 --chunk-id 55 --n-chunks 100
"""
import sys
import os, glob, argparse, pathlib, concurrent.futures as cf
import xarray as xr
import numpy as np
import jdcal
from tqdm import tqdm

# Configuration
DATA_DIR = "data/wfmd_dataset"  # Path to WFMD NetCDF files
OUTFILE  = "methane_all01.txt"   # Output file with extracted measurements

def extract_one(nc_path_and_start_idx):
    """
    Extract methane and CO data from a single NetCDF file.
    
    Processes one file by reading time, coordinates, and measurement variables,
    converting timestamps to Julian Day format, and formatting output for
    downstream analysis.
    
    Args:
        nc_path_and_start_idx: Tuple of (file_path, global_start_index)
        
    Returns:
        Tuple of (formatted_data_rows, row_count)
    """
    nc_path, start_idx = nc_path_and_start_idx
    
    try:
        # Load NetCDF dataset
        ds = xr.open_dataset(nc_path)
        
        # Convert timestamps to Julian Day for consistency
        # Ensures uniform time representation across all files
        time = ds['time'].values
        times = []
        for i in range(len(time)):
            time2 = str(time[i])
            # Convert calendar date to Julian Day
            times.append(jdcal.gcal2jd(time2[0:4], time2[5:7], time2[8:10])[1])
        
        # Extract measurement variables and coordinates
        # Includes methane, CO concentrations, uncertainties, and spatial coordinates
        xch4 = ds['xch4'].values
        xch4u = ds['xch4_uncertainty'].values
        xco = ds['xco'].values
        xcou = ds['xco_uncertainty'].values
        latitude = ds['latitude'].values
        longitude = ds['longitude'].values
        
        # Format output rows with global indexing
        # Each row contains all variables for one measurement point
        rows = []
        for i in range(len(xch4)):
            global_idx = start_idx + i
            # Format as CSV row with global index
            line = f"{global_idx}, {times[i]}, {longitude[i]}, {latitude[i]}, " \
                   f"{xch4[i]}, {xch4u[i]}, {xco[i]}, {xcou[i]}\n"
            rows.append(line)
        
        # Clean up resources
        ds.close()
        return rows, len(xch4)
        
    except Exception as e:
        print(f"Error processing {nc_path}: {e}")
        return [], 0

def main(n_workers: int = 8, chunk_id: int = 0, n_chunks: int = 1):
    """
    Main processing function for parallel NetCDF extraction.
    
    Coordinates file discovery, parallel processing, and output management.
    Supports chunked processing for distributed computing environments.
    
    Args:
        n_workers: Number of parallel processing workers
        chunk_id: Current chunk identifier for distributed processing  
        n_chunks: Total number of processing chunks
    """
    # Discover all NetCDF files recursively
    files = sorted(glob.glob(os.path.join(DATA_DIR, "**/*.nc"), recursive=True))
    if n_chunks > 1:
        files = np.array_split(files, n_chunks)[chunk_id]
    
    # Pre-calculate file sizes for global indexing
    # Ensures unique global indices across all measurements
    print("Pre-calculating file sizes for global indexing...")
    file_sizes = []
    for file_path in tqdm(files, desc="Calculating sizes"):
        try:
            with xr.open_dataset(file_path) as ds:
                file_sizes.append(len(ds['xch4'].values))
        except:
            file_sizes.append(0)
    
    # Calculate cumulative start indices for global indexing
    start_indices = np.cumsum([0] + file_sizes[:-1])
    file_data = list(zip(files, start_indices))
    
    # Determine write mode for chunked processing
    write_mode = 'w' if chunk_id == 0 else 'a'
    if write_mode == 'w':
        pathlib.Path(OUTFILE).unlink(missing_ok=True)
    
    # Execute parallel processing
    print(f"Processing {len(files)} files with {n_workers} workers...")
    with open(OUTFILE, write_mode) as fh:
        with cf.ProcessPoolExecutor(max_workers=n_workers) as executor:
            for rows, _ in tqdm(executor.map(extract_one, file_data), total=len(files)):
                fh.writelines(rows)

if __name__ == "__main__":
    # Command line interface setup
    p = argparse.ArgumentParser(description="Extract methane data from NetCDF files")
    p.add_argument("--workers",  type=int, 
                   default=int(os.environ.get("SLURM_CPUS_ON_NODE", 200)))
    p.add_argument("--chunk-id", type=int, 
                   default=int(os.environ.get("SLURM_ARRAY_TASK_ID", 0)))
    p.add_argument("--n-chunks", type=int, 
                   default=int(os.environ.get("SLURM_ARRAY_TASK_COUNT", 1)))
    args = p.parse_args()
    main(args.workers, args.chunk_id, args.n_chunks)
\end{lstlisting}

\section{Step 2: Land Cover Classification Addition}
\label{sec:appendixB_step2}

The second step adds land cover classification data to each measurement using Google Earth Engine's MODIS land cover dataset. This algorithm processes data in chunks to handle the large dataset efficiently while maintaining reliability.

\begin{lstlisting}[language=Python, caption=Land Cover Classification Addition Algorithm, label=alg:step02_add_lc, breaklines=true]
#!/usr/bin/env python3
"""
Land cover classification addition using Google Earth Engine.
Adds MODIS land cover data to methane measurements by querying
spatial and temporal Earth Engine collections.

Implements chunked processing and robust error handling for 
large-scale datasets with automatic retry logic.
"""

from __future__ import annotations
import time, jdcal, argparse, logging
from pathlib import Path
from collections import defaultdict
import numpy as np, pandas as pd, ee
from tqdm import tqdm

# Processing configuration
SCALE = 3_500          # Earth Engine spatial resolution (meters)
CHUNK_SIZE = 5_000     # Measurements per processing chunk
MAX_RETRIES = 5        # Maximum retry attempts for failed queries
RETRY_DELAY = 4        # Delay between retries (seconds)

# Logging configuration
logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s  %(levelname)s | %(message)s",
                    datefmt="%H:%M:%S")
LOG = logging.getLogger("land_cover_classification")

# Performance tracking utilities
tstat: dict[str,list[float]] = defaultdict(list)
tic = time.perf_counter
def toc(t0: float, key: str): tstat[key].append(time.perf_counter() - t0)
def show_timing():
    """Display timing statistics for performance monitoring."""
    print("\nTiming breakdown:")
    tot = tstat["TOTAL_RUN"][0] if tstat["TOTAL_RUN"] else 0
    for k,v in tstat.items():
        if k=="TOTAL_RUN": continue
        s=len(v) and sum(v)
        print(f"{k:>12s}: {s:6.1f}s * {len(v)} calls * mean {s/len(v):.3f}s")
    print(f"TOTAL_RUN : {tot:.1f}s")

# Initialize Google Earth Engine
try: 
    ee.Initialize()
except Exception:
    ee.Authenticate(); ee.Initialize()
LOG.info("Earth Engine initialized successfully")

# Load MODIS land cover collection
# Provides global land cover classifications at 500m resolution
lc_collection = ee.ImageCollection('MODIS/006/MCD12Q1')

def process_chunk_karoff(df: pd.DataFrame) -> pd.DataFrame:
    """
    Process measurement chunk and add land cover classifications.
    
    Queries Earth Engine for MODIS land cover data at measurement coordinates
    within temporal windows. Implements spatial and temporal filtering with
    robust error handling for API reliability.
    
    Args:
        df: DataFrame with measurement coordinates and timestamps
        
    Returns:
        DataFrame with added land cover classification column
    """
    t0 = tic()
    
    # Handle empty chunks
    if df.empty:
        LOG.warning("Empty chunk encountered, skipping")
        return df
    
    # Extract coordinates and time data
    timel = df['times']
    x = df['lon']
    y = df['lat']
    df.insert(8, 'lc', 0)  # Initialize land cover column
    xy = []
    
    # Define temporal window for land cover query (plus/minus 183 days)
    # Captures seasonal variations in land cover
    time1 = timel.iloc[0] - 183
    time2 = timel.iloc[0] + 183
    
    # Convert Julian Day to calendar dates for Earth Engine
    y1 = jdcal.jd2gcal(2400000.5, time1)[0]
    m1 = jdcal.jd2gcal(2400000.5, time1)[1]
    d1 = jdcal.jd2gcal(2400000.5, time1)[2]
    y2 = jdcal.jd2gcal(2400000.5, time2)[0]
    m2 = jdcal.jd2gcal(2400000.5, time2)[1]
    d2 = jdcal.jd2gcal(2400000.5, time2)[2]
    
    # Format dates for Earth Engine query
    i_date = f"{y1}-{m1:02d}-{d1:02d}"
    f_date = f"{y2}-{m2:02d}-{d2:02d}"
    
    # Build coordinate list for spatial query
    for i in df.index:
        xy.append([x[i], y[i]])
    
    # Create Earth Engine geometry and filter collection
    poi = ee.Geometry.MultiPoint(xy)
    lc_poi = lc_collection.filterDate(i_date, f_date).select('LC_Type1')
    
    # Query Earth Engine with retry logic
    for attempt in range(MAX_RETRIES):
        try:
            # Execute land cover query
            lc_r_poi = lc_poi.getRegion(poi, SCALE).getInfo()
            
            # Verify sufficient data returned
            if len(lc_r_poi) > len(df):
                # Assign land cover values to measurements
                for i in range(len(df)):
                    if i + 1 < len(lc_r_poi):
                        df.at[df.index[i], 'lc'] = lc_r_poi[i+1][4]
                
                toc(t0, "chunk_processing")
                LOG.debug(f"Successfully processed chunk of {len(df)} measurements, "
                         f"got {len(lc_r_poi)} EE responses")
                return df
            else:
                # Insufficient data returned, keep default values
                LOG.debug(f"Insufficient EE responses: got {len(lc_r_poi)}, "
                         f"expected > {len(df)}")
                toc(t0, "chunk_processing")
                return df
                    
        except Exception as e:
            LOG.warning(f"EE query failed (attempt {attempt+1}/{MAX_RETRIES}): {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY * (attempt + 1))
            else:
                LOG.error(f"Failed to process chunk after {MAX_RETRIES} attempts")
                break
    
    # Return chunk with default values if all retries failed
    toc(t0, "chunk_processing")
    return df

def main(src: Path, dst: Path):
    """
    This is our main orchestrator - the conductor of our land cover symphony!
    
    It takes our big file of methane measurements and works through it chunk by chunk,
    adding land cover information to each measurement. It's smart enough to pick up 
    where it left off if something goes wrong (like if your computer crashes), and 
    it keeps track of progress so you know how it's doing.
    
    What we need:
        src: The file with all our methane measurements (the input)
        dst: Where we want to save the results with land cover added (the output)
    """
    LOG.info("Let's add some land cover magic to our methane data!")
    
    # These are the columns we expect in our input file
    headernames = ['index', 'times', 'lon', 'lat', 'xch4', 'xch4u', 'xco', 'xcou']
    
    # Starting our performance timer and counters
    tot0 = tic()  # When did we start?
    tstat["TOTAL_RUN"].append(0)
    processed_chunks = 0      # How many chunks have we finished?
    total_measurements = 0    # How many measurements have we processed?
    
    # Smart resume feature - if we've already started this job, pick up where we left off
    skip_chunks = 0
    if dst.exists():  # Do we already have some results?
        try:
            existing_df = pd.read_csv(dst)
            existing_rows = len(existing_df)
            skip_chunks = existing_rows // CHUNK_SIZE
            remaining_rows = existing_rows % CHUNK_SIZE
            
            if remaining_rows > 0:
                # Let's clean up any partial chunk to avoid confusion
                clean_rows = skip_chunks * CHUNK_SIZE
                existing_df.iloc[:clean_rows].to_csv(dst, index=False)
                LOG.info(f"Found some partial work - keeping {clean_rows} good rows, "
                        f"will redo {remaining_rows} partial ones")
            else:
                LOG.info(f"Great! We already have {existing_rows} rows done, "
                        f"so we'll skip ahead {skip_chunks} chunks")
        except Exception as e:
            LOG.warning(f"Couldn't read the existing file, so we'll start fresh: {e}")
            skip_chunks = 0
            dst.unlink(missing_ok=True)  # Clean slate!
    
    # Let's read and process our data in bite-sized chunks (easier on memory!)
    chunk_iterator = pd.read_csv(src, chunksize=CHUNK_SIZE, header=None, names=headernames)
    
    for chunk_idx, df in enumerate(tqdm(chunk_iterator, desc="Processing chunks")):
        if df.empty:
            continue
            
        # If we're resuming, we can skip chunks we already finished
        if chunk_idx < skip_chunks:
            continue  # "Been there, done that!"
            
        # Time to work on this chunk - add land cover info to all measurements!
        processed_df = process_chunk_karoff(df)
        
        # Save our progress! First chunk creates the file, others append to it
        if processed_chunks == 0 and skip_chunks == 0:
            processed_df.to_csv(dst, mode='w', header=True, index=False)   # New file with headers
        else:
            processed_df.to_csv(dst, mode='a', header=False, index=False)  # Add to existing file
        
        processed_chunks += 1
        total_measurements += len(processed_df)
        
        # Let's celebrate our progress every 10 chunks!
        if processed_chunks % 10 == 0:
            LOG.info(f"Making good progress: {processed_chunks} chunks done, "
                    f"{total_measurements:,} measurements processed!")
    
    # Time to see how fast we were!
    tstat["TOTAL_RUN"][0] = time.perf_counter() - tot0
    
    # Victory lap time! Let's see what we accomplished
    LOG.info("Land cover classification is complete!")
    LOG.info(f"  We processed {processed_chunks} chunks")
    LOG.info(f"  That's {total_measurements:,} measurements with land cover data!")
    LOG.info(f"  Results saved to: {dst}")

# Setting up our command line interface - making it easy to run from anywhere!
ap = argparse.ArgumentParser(
    description="Add land cover classification to methane data")
ap.add_argument("--src", type=Path, default="methane_all01.txt", 
                help="Where to find our methane measurements")  # The input file
ap.add_argument("--dst", type=Path, default="methane_all02.txt",
                help="Where to save results with land cover added")  # The output file
ap.add_argument("--verbose", "-v", action="store_true",
                help="Enable verbose logging")
args = ap.parse_args()

if args.verbose:
    logging.getLogger().setLevel(logging.DEBUG)

if __name__ == "__main__":
    main(args.src, args.dst)
    show_timing()
\end{lstlisting}

\section{Step 3: Histogram Generation by Continent and Season}
\label{sec:appendixB_step3}

The third step generates histograms of methane and carbon monoxide measurements grouped by continent and season. This algorithm uses a streaming approach to handle large datasets efficiently.

\begin{lstlisting}[language=Python, caption=Histogram Generation Algorithm, label=alg:step03_histos, breaklines=true]
#!/usr/bin/env python3
"""
Histogram generation for methane and CO measurements by continent and season.
Creates statistical distributions grouped by geographical and temporal classifications
using streaming processing for memory efficiency.
"""

from __future__ import annotations

import argparse
import math
import os
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from joblib import Parallel, delayed
from tqdm import tqdm

# Configure matplotlib for publication-quality figures
plt.rcParams.update(
    {"figure.figsize": (6, 4), "axes.grid": True, "font.size": 9}
)

# Geographic and temporal classification parameters
_CONTINENTS = [
    "Africa",
    "Asia",
    "Australia",
    "North America",
    "Oceania",
    "South America",
    "Antarctica",
    "Europa",
    "World",
]
_SEASON_LABELS = ["Winter", "Spring", "Summer", "Fall"]
# Season boundaries in Modified Julian Day (solstice/equinox dates)
_SEASON_MJD = [58108, 58197, 58290, 58384, 58473]

# Variable specifications for histogram binning
_VARS = ["xch4", "xco"]
_BIN_SPEC = {
    "xch4": dict(range=(1500, 2500), bins=200),  # Methane concentration range
    "xco": dict(range=(0, 300), bins=150),       # Carbon monoxide concentration range
}

def _season_of_mjd(mjd: float) -> int:
    """
    Determine season for given Modified Julian Day.
    
    Maps MJD values to seasonal indices using predefined boundaries
    based on solstice and equinox dates.
    
    Args:
        mjd: Modified Julian Day value
        
    Returns:
        Integer 0-3 for Winter-Fall (Northern Hemisphere)
    """
    for i in range(4):
        if _SEASON_MJD[i] <= mjd < _SEASON_MJD[i + 1]:
            return i
    return 0  # Default to Winter for edge cases

def _simplify_shp(shp: Path, percent: float = 0.01) -> gpd.GeoSeries:
    """
    Load our continent shapes and make them simpler for faster processing.
    
    Think of it like going from a super detailed map to a simplified one - we remove
    unnecessary detail while keeping the overall shape accurate. This makes our
    computer much happier when processing millions of points!
    
    What we need:
        shp: Where to find our continent boundary file
        percent: How much to simplify (1% = remove 99% of the detail points)
        
    What we give back:
        Simplified continent shapes that are much faster to work with
    """
    poly = gpd.read_file(shp).to_crs("EPSG:4326")  # Load and standardize coordinate system
    # Figure out how much detail we can safely remove
    minx, miny, maxx, maxy = poly.total_bounds  # Get the overall boundary
    diag = math.hypot(maxx - minx, maxy - miny)  # Calculate diagonal distance
    tol = diag * percent  # Our simplification threshold
    return poly.geometry.simplify(tol, preserve_topology=True)  # Simplify while keeping shape integrity

def _make_empty_hist() -> Dict[Tuple[int, int], np.ndarray]:
    """
    Set up empty storage bins for all our histograms.
    
    Like preparing a filing cabinet with folders for every continent-season combo.
    Each folder will eventually hold counts of methane and CO measurements.
    
    What we give back:
        A big nested dictionary with zeros everywhere, ready to be filled with data
    """
    h = {}  # Our empty filing cabinet
    for ci in range(len(_CONTINENTS)):  # For each continent
        for si in range(4):  # For each season
            # Create empty bins for both methane and CO measurements
            h[(ci, si)] = {v: np.zeros(_BIN_SPEC[v]["bins"], int) for v in _VARS}
    return h

def _process_chunk(
    df: pd.DataFrame,
    geoms: gpd.GeoSeries,
    binspec: dict,
) -> Dict[Tuple[int, int], Dict[str, np.ndarray]]:
    """
    This is where the magic happens! Process a chunk of measurements and sort them
    into the right bins.
    
    For each measurement, we figure out: 1) Which continent is it over? 2) What season
    was it taken in? 3) What bin does the methane/CO value belong in? Then we add it
    to our growing histogram counts.
    
    What we need:
        df: A chunk of measurement data with coordinates and values
        geoms: The continent boundary shapes
        binspec: The rules for how to organize our bins
        
    What we give back:
        Updated counts for each continent-season-variable combo
    """
    # Turn our coordinates into geographic points for spatial analysis
    gdf = gpd.GeoDataFrame(df, 
                          geometry=gpd.points_from_xy(df.lon, df.lat), 
                          crs="EPSG:4326")
    out = _make_empty_hist()  # Start with empty bins
    
    # Pre-calculate which points belong to which continents (for speed!)
    world_mask = np.ones(len(df), bool)  # Everything belongs to "World" category
    mask_cache = {}  # Our lookup table
    for ci in range(len(_CONTINENTS) - 1):  # Check all continents except "World"
        mask_cache[ci] = gdf.within(geoms.iloc[ci]).values  # Is this point inside this continent?

    # Now let's go through each measurement and sort it properly
    for idx, row in df.iterrows():
        mjd = int(row.times)  # Get the date
        si = _season_of_mjd(mjd)  # What season was this taken in?
        
        # Figure out which continent this measurement belongs to
        for ci in range(len(_CONTINENTS)):
            # Special case: "World" includes everything, others use our pre-calculated masks
            mask = world_mask if ci == len(_CONTINENTS) - 1 else mask_cache[ci]
            if not mask[idx]:  # This point doesn't belong to this continent
                continue  # Skip to the next continent
                
            # Time to bin this measurement! (Sort it into the right histogram bucket)
            for v in _VARS:  # For both methane and CO
                spec = binspec[v]  # Get the binning rules for this variable
                bin_edges = np.linspace(*spec["range"], spec["bins"] + 1)  # Create the bin boundaries
                val = row[v]  # The actual measurement value
                
                # Only count values that are in our expected range
                if spec["range"][0] <= val < spec["range"][1]:
                    bi = np.searchsorted(bin_edges, val, side="right") - 1  # Find which bin this belongs in
                    out[(ci, si)][v][bi] += 1  # Add one to that bin's count!
    return out

def _merge_hist(a: Dict, b: Dict):
    """
    Combine two sets of histogram counts - like merging two photo albums!
    
    When we process data in parallel (multiple workers at once), each worker
    creates their own histogram counts. This function adds them all together
    to get the final totals.
    
    What we need:
        a: First set of histogram counts (this one gets updated)
        b: Second set of histogram counts to add in
    """
    # Go through each continent-season combination and add the counts together
    for k in a:  # For each (continent, season) pair
        for v in _VARS:  # For both methane and CO
            a[k][v] += b[k][v]  # Add the bin counts together

def _plot_hist(
    hist: Dict[Tuple[int, int], Dict[str, np.ndarray]],
    outdir: Path,
):
    """
    Time to turn our numbers into beautiful plots!
    
    This creates actual histogram charts for every continent-season-variable combination
    that has data. We only make plots where we actually have measurements (no point in
    showing empty charts). Each plot gets saved as a file with a descriptive name.
    
    What we need:
        hist: All our carefully counted histogram data
        outdir: Where to save all the pretty plots
    """
    outdir.mkdir(parents=True, exist_ok=True)  # Make sure our output folder exists
    
    # Let's make plots for every continent!
    for ci, cont in enumerate(_CONTINENTS):
        for si, sname in enumerate(_SEASON_LABELS):
            for v in _VARS:
                counts = hist[(ci, si)][v]
                
                # Skip empty histograms
                if counts.sum() == 0:
                    continue
                    
                # Create histogram plot
                spec = _BIN_SPEC[v]
                bin_edges = np.linspace(*spec["range"], spec["bins"] + 1)
                centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])
                
                plt.figure()
                plt.bar(centers, counts, 
                        width=centers[1] - centers[0], 
                        edgecolor="none")
                plt.title(f"{v.upper()} - {cont} - {sname}")
                plt.xlabel(v.upper())
                plt.ylabel("counts")
                
                # Save figure with descriptive filename
                fname = f"{v}_{cont.replace(' ', '_')}_{sname}.png"
                plt.tight_layout()
                plt.savefig(outdir / fname, dpi=150)
                plt.close()

def main():
    """
    The grand conductor of our histogram symphony!
    
    This orchestrates the whole process: loads continent shapes, reads our methane data
    in chunks (so we don't run out of memory), processes everything either in parallel
    or sequentially, and finally creates all the beautiful histogram plots.
    """
    # Setting up our command line options - making it user-friendly!
    pa = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    pa.add_argument("--csv", required=True, type=Path, 
                   help="processed methane_all02.txt")
    pa.add_argument("--shp", required=True, type=Path, 
                   help="continent shapefile")
    pa.add_argument("--fig", type=Path, default=Path("fig"), 
                   help="output figure folder")
    pa.add_argument("--chunksize", type=int, default=200_000, 
                   help="rows per read chunk")
    pa.add_argument("--workers", type=int, default=1, 
                   help="parallel histogram workers")
    args = pa.parse_args()

    # First, let's get our continent shapes ready for fast processing
    print("Loading and simplifying continent polygons...")
    geoms = _simplify_shp(args.shp)  # Load and simplify for speed

    # Now for the main event - processing our data in manageable chunks
    print("Creating histograms from streaming data...")
    hist = _make_empty_hist()  # Start with empty bins
    rdr = pd.read_csv(args.csv, chunksize=args.chunksize, 
                     usecols=["times", "lon", "lat", "xch4", "xco"])
    
    # Choose our processing strategy based on how many workers we have
    if args.workers > 1:
        # Multiple workers = parallel processing (faster for big datasets!)
        results = Parallel(n_jobs=args.workers, backend="loky")(
            delayed(_process_chunk)(df, geoms, _BIN_SPEC) for df in tqdm(rdr)
        )
        for h in results:
            _merge_hist(hist, h)  # Combine all the results
    else:
        # One worker = process chunk by chunk (easier to debug)
        for df in tqdm(rdr):
            h = _process_chunk(df, geoms, _BIN_SPEC)
            _merge_hist(hist, h)  # Add this chunk's results to our totals

    # Time to create all our beautiful plots!
    print("Creating and saving histogram figures...")
    _plot_hist(hist, args.fig)
    print(f"All done! Check out your figures in {args.fig}")

if __name__ == "__main__":
    main()
\end{lstlisting}

\section{Step 4: Heatmap Generation}
\label{sec:appendixB_step4}

The fourth step creates a color-coded matrix heatmap visualization of the methane measurements. This algorithm processes the matrix data and generates publication-quality figures.

\begin{lstlisting}[language=Python, caption=Heatmap Generation Algorithm, label=alg:step04_heatmap, breaklines=true]
#!/usr/bin/env python
"""
Heatmap generation for methane concentration matrices.
Creates color-coded visualizations showing spatial and temporal
patterns in methane concentration data.
"""

import seaborn as sb, matplotlib.pyplot as plt, pandas as pd, argparse, pathlib

def main(csv_path="matrix.csv", out_png="fig/matrix.png", cmap="rocket_r"):
    """
    Generate heatmap visualization from matrix data.
    
    Creates publication-quality heatmap with color-coded representation
    of methane concentrations across spatial and temporal dimensions.
    
    Args:
        csv_path: Path to input matrix CSV file
        out_png: Path for output PNG figure
        cmap: Colormap for visualization
    """
    # Load matrix data
    df = pd.read_csv(csv_path, index_col=0)
    df.index = df.index.astype(str)
    
    # Create output directory
    pathlib.Path("fig").mkdir(exist_ok=True)
    
    # Create figure with appropriate size
    plt.figure(figsize=(18,18))
    
    # Generate heatmap with annotations and color bar
    sb.heatmap(df, annot=True, fmt=".0f",
               cbar_kws={"orientation": "horizontal","shrink":0.8,
                         "label":"XCH$_4$ [ppb]"},
               cmap=cmap)
    
    # Save figure with high resolution
    plt.savefig(out_png, dpi=300)

if __name__ == "__main__":
    # Command line interface
    p = argparse.ArgumentParser(description="Generate methane concentration heatmap")
    p.add_argument("--csv",  default="matrix.csv")
    p.add_argument("--out",  default="fig/matrix.png")
    args = p.parse_args()
    main(args.csv, args.out)
\end{lstlisting}

\section{Step 5: Dataset Health Check and Validation}
\label{sec:appendixB_step5}

The fifth step provides comprehensive dataset validation and health checking capabilities. This algorithm performs both file-level and row-level audits to ensure data quality and completeness.

\begin{lstlisting}[language=Python, caption=Dataset Health Check Algorithm, label=alg:step05_health_check, breaklines=true]
#!/usr/bin/env python3
"""
WFMD dataset health-check
Comprehensive validation tool for WFMD dataset integrity and completeness.
Performs both file-level and row-level audits to ensure data quality
before running the main analysis workflow.

Key features:
- Per-month file counts, date span and completeness flags
- Min/max sanity-sampling of xch4/xco/lat/lon
- Optional row-level audit (continent x season counts, LC distribution)
- All diagnostics in a single, self-contained script
"""
from __future__ import annotations

import argparse, datetime as dt, random, re, textwrap
from collections import defaultdict
from pathlib import Path

import numpy as np
import pandas as pd
import xarray as xr

try:
    import geopandas as gpd
except ImportError:
    gpd = None

# Constants used throughout the pipeline for consistency
_CONTINENTS = ["Africa","Asia","Australia","North America",
               "Oceania","South America","Antarctica","Europa","World"]
_SEASON_LABELS = ["Spring","Summer","Fall","Winter","All Seasons"]
_SEASONS = [58108,58197,58290,58384,58473,58562,58655,58749,58839,58928,59020,59114]
_MJD0 = dt.date(1858,11,17)
_MONTH_RE = re.compile(r"WFMD-(\d{6})$")
_EXPECTED_FILES_PER_MONTH = 25

def _season_of(mjd:int)->int:
    """
    Determine the season for a given Modified Julian Day.
    
    Args:
        mjd: Modified Julian Day value
        
    Returns:
        Integer 0-4 for Spring-All Seasons classification
    """
    for k in range(4):
        if (_SEASONS[k]   < mjd < _SEASONS[k+1] or
            _SEASONS[k+4] < mjd < _SEASONS[k+5]):
            return k
    return 4  # Default to "All Seasons" for edge cases

def _open_mjd(nc:Path)->float|None:
    """
    Safely open a NetCDF file and extract the Modified Julian Day.
    
    Args:
        nc: Path to NetCDF file
        
    Returns:
        Modified Julian Day as float, or None if file cannot be processed
    """
    try:
        with xr.open_dataset(nc, decode_cf=False) as ds:
            return float(ds["time"].values)
    except Exception:
        return None

def audit_files(root:Path, sample_per_month:int=3)->pd.DataFrame:
    """
    Perform comprehensive file-level audit of the WFMD dataset.
    
    This function examines the dataset structure, file counts, date ranges,
    and samples data values to ensure the dataset is complete and valid.
    
    Args:
        root: Root directory of the WFMD dataset
        sample_per_month: Number of files to sample per month for value ranges
        
    Returns:
        DataFrame containing audit results and quality metrics
    """
    root = Path(root)
    rec_cnt, t_min, t_max, bad_ts = defaultdict(int), {}, {}, {}
    sample_stats:dict[str,dict[str,tuple[float,float]]] = defaultdict(
        lambda: defaultdict(lambda:(np.inf,-np.inf)))

    # Iterate through all directories to find monthly data folders
    for folder in sorted(root.rglob("*")):
        if not folder.is_dir(): 
            continue
        m = _MONTH_RE.search(str(folder))
        if not m: 
            continue
        yymm = m.group(1)
        ncs  = sorted(folder.glob("*.nc"))
        rec_cnt[yymm] = len(ncs)
        if not ncs:
            continue

        # Analyze temporal coverage by examining first and last files
        for nc in (ncs[0], ncs[-1]):
            mjd = _open_mjd(nc)
            if mjd is None:
                continue
            d = _MJD0 + dt.timedelta(days=mjd)
            t_min[yymm] = d if yymm not in t_min or d<t_min[yymm] else t_min[yymm]
            t_max[yymm] = d if yymm not in t_max or d>t_max[yymm] else t_max[yymm]
        
        # Check for timestamp inconsistencies
        bad_ts[yymm] = (
            (yymm in t_min and t_min[yymm].strftime("%Y%m")!=yymm) or
            (yymm in t_max and t_max[yymm].strftime("%Y%m")!=yymm)
        )

        # Sample data values to establish value ranges for quality control
        sample_pick = random.sample(ncs, k=min(sample_per_month, len(ncs)))
        for nc in sample_pick:
            try:
                with xr.open_dataset(nc) as ds:
                    for var in ("xch4","xco","latitude","longitude"):
                        if var not in ds:
                            continue
                        v = ds[var].values.astype(float)
                        if v.size == 0:
                            continue
                        lo, hi = np.nanmin(v), np.nanmax(v)
                        vlo, vhi = sample_stats[yymm][var]
                        sample_stats[yymm][var] = (min(vlo,lo), max(vhi,hi))
            except Exception:
                continue

    # Compile audit results into a comprehensive DataFrame
    df = pd.DataFrame({"n_nc":rec_cnt,"t_min":t_min,"t_max":t_max,"bad_ts":bad_ts}
                      ).sort_index()
    df.index.name = "YYYYMM"
    df["YYYY"] = df.index.str[:4]; df["MM"] = df.index.str[4:]

    # Add completeness flags for quality assessment
    df["n_nc_ok"] = df["n_nc"] >= _EXPECTED_FILES_PER_MONTH
    df["t_min"]   = pd.to_datetime(df["t_min"])
    df["t_max"]   = pd.to_datetime(df["t_max"])
    df["day_span"]    = (df["t_max"] - df["t_min"]).dt.days + 1
    df["day_span_ok"] = df["day_span"] >= 25

    # Attach sampled value ranges for data quality assessment
    if sample_stats:
        wide = pd.DataFrame.from_dict(
            {(m,v): sample_stats[m][v] for m in sample_stats for v in sample_stats[m]},
            orient="index", columns=["v_min","v_max"])
        df = pd.concat([df, wide.unstack(level=1)], axis=1)

    return df

def audit_rows(csv:Path, shp:Path):
    """
    Perform detailed row-level audit of processed methane data.
    
    This function analyzes the processed dataset to verify data quality,
    spatial distribution, temporal coverage, and land cover classification.
    
    Args:
        csv: Path to processed CSV file
        shp: Path to continent shapefile
    """
    if gpd is None:
        print("\nGeopandas not installed - row-level audit skipped"); return
    try:
        df = pd.read_csv(csv)
        if df.empty or df.shape[1]==0:
            raise ValueError("CSV appears empty or header-only")
    except Exception as e:
        print(f"\nCannot read {csv}: {e}"); return

    # Display comprehensive statistical summary of numerical data
    print("\nGlobal numeric stats:")
    print(df.select_dtypes(include=[np.number]).describe().T)
    dates = pd.to_datetime(df.times, unit="D", origin="1858-11-17")
    print("Date span:", dates.min().date(), "->", dates.max().date())

    # Analyze land cover classification distribution
    if "lc" in df:
        print("\nLC_Type1 value counts:")
        print(df["lc"].value_counts(sort=False))
    else:
        print("\n(column 'lc' missing - run step02 first)")

    # Perform spatial analysis using continent shapefiles
    gdf = gpd.GeoDataFrame(df, 
                          geometry=gpd.points_from_xy(df.lon, df.lat), 
                          crs="EPSG:4326")
    poly = gpd.read_file(shp).to_crs("EPSG:4326")

    # Count measurements by continent and season
    cnt = np.zeros((9,5), int)
    for ci, cont in enumerate(_CONTINENTS):
        mask = np.ones(len(df),bool) if cont=="World" else gdf.within(poly.geometry[ci]).values
        for mjd in df.times[mask].astype(int):
            cnt[ci, _season_of(mjd)] += 1
    print("\nRows per continent x season:")
    print(pd.DataFrame(cnt, index=_CONTINENTS, columns=_SEASON_LABELS))

    # Verify data value ranges for quality control
    print("\nXCH4 / XCO ranges:")
    for v in ("xch4","xco"):
        if v in df: 
            print(f"{v.upper():5s}: {df[v].min():8.1f} ... {df[v].max():8.1f}")

    # Check for missing data in each column
    print("\nMissing per column:")
    print(df.isna().sum())

if __name__ == "__main__":
    # Set up command line interface with comprehensive help
    pa = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=textwrap.dedent(__doc__))
    pa.add_argument("--root", required=True, type=Path, help="WFMD dataset root dir")
    pa.add_argument("--csv",  type=Path, help="processed methane_all02.txt")
    pa.add_argument("--shp",  type=Path, help="world-continents shapefile")
    pa.add_argument("--sample", type=int, default=3,
                    help="NetCDFs sampled per month for min/max stats (default 3)")
    a = pa.parse_args()

    # Perform file-level audit and display results
    df_months = audit_files(a.root, a.sample)
    print("\nFile counts & time span per month:\n")
    keep_cols = ["n_nc","n_nc_ok","day_span","day_span_ok","t_min","t_max","bad_ts"]
    print(df_months[keep_cols])

    # Display temporal coverage summary
    print("\nPivot (years x months):\n")
    print(df_months.pivot_table(index="YYYY", columns="MM", values="n_nc",
                                fill_value=0).astype(int))

    # Perform optional row-level audit if both CSV and shapefile are provided
    if a.csv and a.shp:
        audit_rows(a.csv, a.shp)
    elif a.csv or a.shp:
        print("\n(Row-level audit needs BOTH --csv and --shp) - skipped")
\end{lstlisting}

\section{Algorithm Summary and Workflow Integration}
\label{sec:appendixB_summary}

The complete workflow consists of five main steps that process methane detection data from raw NetCDF files to final analysis and visualization:

\begin{enumerate}
	\item \textbf{Data Extraction} (Step 1): Parallel extraction of methane and carbon monoxide measurements from WFMD NetCDF files with proper time conversion and global indexing.

	\item \textbf{Land Cover Classification} (Step 2): Addition of MODIS land cover data using Google Earth Engine, processing data in chunks with enhanced error handling and reliability.

	\item \textbf{Histogram Generation} (Step 3): Creation of continent and season-specific histograms for methane and carbon monoxide measurements using streaming processing for large datasets.

	\item \textbf{Heatmap Visualization} (Step 4): Generation of color-coded matrix heatmaps for publication-quality methane concentration visualizations.

	\item \textbf{Data Validation} (Step 5): Comprehensive dataset health checking including file-level audits, row-level validation, and quality assessment.
\end{enumerate}

Each algorithm is designed to handle large-scale datasets efficiently through parallel processing, streaming data handling, and memory-conscious operations. The workflow maintains data integrity while providing robust error handling and comprehensive logging for scientific reproducibility. All scripts and additional implementation details are available in the public repository: \url{https://github.com/marcoruizrueda-ist/PHD}.
